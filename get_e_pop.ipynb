{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def project_onto_direction(H, direction):\n",
    "    \"\"\"\n",
    "    Projects the given gcn_embeddings onto the given direction.\n",
    "\n",
    "    Args:\n",
    "        H (torch.Tensor): The gcn_embeddings to project.(n, embedding_dim)\n",
    "        direction (torch.Tensor): The direction to project onto.(embedding_dim,)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The projected states.(n, )\n",
    "    \"\"\"\n",
    "    if not isinstance(H, torch.Tensor):\n",
    "        H = torch.tensor(H, dtype=torch.float32, device='cuda')  \n",
    "\n",
    "    if not isinstance(direction, torch.Tensor):\n",
    "        direction = torch.tensor(direction, dtype=torch.float32, device=H.device)  \n",
    "\n",
    "    mag = torch.norm(direction)\n",
    "    assert not torch.isinf(mag).any(), \"direction is inf\"\n",
    "\n",
    "    # calculate the projection\n",
    "    projection = H.matmul(direction) / mag\n",
    "    return projection\n",
    "\n",
    "def recenter(x, mean=None):\n",
    "    \"\"\"\n",
    "    Recenter the given data to have zero mean.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The data to recenter.(n, embedding_dim)\n",
    "        mean (torch.Tensor, optional): The mean to subtract from the data. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The recentered data.(n, embedding_dim)\n",
    "    \"\"\"\n",
    "    x = torch.Tensor(x).cuda()\n",
    "    if mean is None:\n",
    "        mean = torch.mean(x, axis=0, keepdims= True).cuda()\n",
    "    else:\n",
    "        mean = torch.Tensor(mean).cuda()\n",
    "    return x - mean\n",
    "\n",
    "class RepReader(ABC):\n",
    "    \"\"\"Class to identify and store concept directions.\n",
    "    \n",
    "    Subclasses implement the abstract methods to identify concept directions \n",
    "    for each hidden layer via strategies including PCA, and cluster means.\n",
    "\n",
    "    RepReader instances are used by RepReaderPipeline to get concept scores.\n",
    "\n",
    "    Directions can be used for downstream interventions.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        self.contrast = True\n",
    "        self.directions = None # directions accessible via directions[layer]\n",
    "        self.direction_signs = None # direction of high concept scores (mapping min/max to high/low)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_rep_directions(self, gcn_embeddings: torch.Tensor) -> None:\n",
    "        \"\"\"Identify concept directions for each embeddings.\n",
    "        \n",
    "        Args:\n",
    "            H (torch.Tensor): The embeddings to analyze.(n, embedding_dim)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_signs(self, gcn_embeddings: torch.Tensor, train_labels) -> None:\n",
    "        \"\"\"Identify the direction of high concept scores.\n",
    "        \n",
    "        Args:\n",
    "            gcn_embeddings (torch.Tensor): The embeddings to analyze.(n, embedding_dim)\n",
    "            train_labels (torch.Tensor): The labels to analyze.(n, )\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def test_accuracy(self, gcn_embeddings: torch.Tensor, test_labels, direction_signs) -> None:\n",
    "        \"\"\"Test the accuracy of the concept directions.\n",
    "        \n",
    "        Args:\n",
    "            gcn_embeddings (torch.Tensor): The embeddings to analyze.(n, embedding_dim)\n",
    "            test_labels (torch.Tensor): The labels to analyze.(n, )\n",
    "            direction_signs (dict): The direction of high concept scores.(n, )\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class PCARepReader(RepReader):\n",
    "    \"\"\" Extract directions via PCA.\"\"\"\n",
    "\n",
    "    def __init__(self, n_components: int = 1, contrast: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.n_components = n_components\n",
    "        self.contrast = contrast\n",
    "        self.H_train_means = {}\n",
    "    \n",
    "    def get_rep_directions(self, gcn_embeddings: torch.Tensor) -> dict:\n",
    "        \"\"\"Identify concept directions for each embeddings.\n",
    "        \n",
    "        Args:\n",
    "            gcn_em (torch.Tensor): The embeddings to analyze.(n, layers, embedding_dim)\n",
    "        \"\"\"\n",
    "        self.directions = {}\n",
    "        self.explained_variance_ratios = {}\n",
    "        for layer in range(gcn_embeddings.shape[1]):\n",
    "            H = gcn_embeddings[:, layer, :].clone()\n",
    "            if self.contrast:\n",
    "                H_relative = H[1::2][:] - H[::2][:]\n",
    "                H_mean = torch.mean(H_relative, axis=0, keepdims=True)\n",
    "                self.H_train_means[layer] = H_mean\n",
    "                H_train = recenter(H_relative, H_mean).cpu().numpy()\n",
    "            else:\n",
    "                H_mean = torch.mean(H, axis=0, keepdims=True)\n",
    "                self.H_train_means[layer] = H_mean\n",
    "                H_train = recenter(H, H_mean).cpu().numpy()\n",
    "            H_train = np.vstack(H_train)\n",
    "            pca = PCA(n_components=self.n_components)\n",
    "            pca.fit(H_train)\n",
    "            self.directions[layer] = pca.components_\n",
    "            print(\"explained_variance_ratio:\", pca.explained_variance_ratio_)\n",
    "            self.explained_variance_ratios[layer] = pca.explained_variance_ratio_\n",
    "        return self.directions\n",
    "\n",
    "    \n",
    "    def get_signs(self, gcn_embeddings: torch.Tensor, train_labels) -> dict:\n",
    "        \"\"\"Identify the direction of high concept scores.\n",
    "        \n",
    "        Args:\n",
    "            gcn_embeddings (torch.Tensor): The embeddings to analyze.(n, layers, embedding_dim)\n",
    "            train_labels (torch.Tensor): The labels to analyze.(n, )\n",
    "        \"\"\"\n",
    "        self.direction_signs = {}\n",
    "        for layer in range(gcn_embeddings.shape[1]):\n",
    "            H = gcn_embeddings[:, layer, :].clone()\n",
    "            H = recenter(H, self.H_train_means[layer])\n",
    "            layer_signs = np.zeros(self.n_components)\n",
    "            for component_index in range(self.n_components):\n",
    "                trans_states = project_onto_direction(H, self.directions[layer][component_index])\n",
    "                layer_signs[component_index] = np.sign(np.mean(trans_states.cpu().numpy()[train_labels.cpu().numpy() == 1]) - np.mean(trans_states.cpu().numpy()[train_labels.cpu().numpy() == 0]))\n",
    "                if layer_signs[component_index] == 0:\n",
    "                    layer_signs[component_index] = 1\n",
    "            self.direction_signs[layer] = layer_signs\n",
    "        return self.direction_signs\n",
    "\n",
    "    def test_accuracy(self, gcn_embeddings: torch.Tensor, n_compont: int = 0) -> dict:\n",
    "        \"\"\"Test the accuracy of the concept directions.\n",
    "            to be implemented: n_components > 1\n",
    "        Args:\n",
    "            gcn_embeddings (torch.Tensor): The embeddings to analyze.(n, embedding_dim)\n",
    "        \"\"\"\n",
    "        test_results = {}\n",
    "        for layer in range(gcn_embeddings.shape[1]):\n",
    "            H = gcn_embeddings[:, layer, :].clone()\n",
    "            H = recenter(H, self.H_train_means[layer])\n",
    "            H_test = project_onto_direction(H, self.directions[layer][n_compont])\n",
    "            test_results[layer] = H_test.cpu().numpy()\n",
    "        return test_results\n",
    "            \n",
    "\n",
    "class ClusterMeanRepReader(RepReader):\n",
    "    \"\"\" Get the direction that is the difference between the mean of the positive and negative clusters.\"\"\"\n",
    "    n_components = 1\n",
    "    def __init__(self, contrast: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.contrast = contrast\n",
    "        \n",
    "    def get_rep_directions(self, gcn_embeddings: torch.Tensor) -> dict:\n",
    "        \"\"\"Identify concept directions for each embeddings.\n",
    "        \n",
    "        Args:\n",
    "            gcn_em (torch.Tensor): The embeddings to analyze.(n, layers, embedding_dim)\n",
    "        \"\"\"\n",
    "        self.directions = {}\n",
    "        for layer in range(gcn_embeddings.shape[1]):\n",
    "            H = gcn_embeddings[:, layer, :].clone()\n",
    "            if self.contrast:\n",
    "                H_pos = H[1::2][:]\n",
    "                H_neg = H[::2][:]\n",
    "                H_pos_mean = torch.mean(H_pos, axis=0, keepdims=True)\n",
    "                H_neg_mean = torch.mean(H_neg, axis=0, keepdims=True)\n",
    "                self.directions[layer] = H_pos_mean - H_neg_mean\n",
    "            else:\n",
    "                H_mean = torch.mean(H, axis=0, keepdims=True)\n",
    "                self.directions[layer] = H_mean                \n",
    "        return self.directions\n",
    "\n",
    "    \n",
    "    def get_signs(self, gcn_embeddings: torch.Tensor) -> None:\n",
    "        return None\n",
    "\n",
    "    def test_accuracy(self, gcn_embeddings: torch.Tensor) -> None:\n",
    "        \"\"\"Test the accuracy of the concept directions.\n",
    "            to be implemented: n_components > 1\n",
    "        Args:\n",
    "            gcn_embeddings (torch.Tensor): The embeddings to analyze.(n, embedding_dim)\n",
    "        \"\"\"\n",
    "        test_results = {}\n",
    "        for layer in range(gcn_embeddings.shape[1]):\n",
    "            H = gcn_embeddings[:, layer, :].clone()\n",
    "            H_test = project_onto_direction(H, self.directions[layer].reshape(-1, 1))\n",
    "            test_results[layer] = H_test.squeeze().cpu().numpy()\n",
    "        return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963648ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RepReader and PCARepReader classes are defined. You can now use them to extract concept directions from GCN embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53409da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set to the GPU ID you want to use\n",
    "BASE_DIR = '.'\n",
    "from recbole.data import (\n",
    "    create_dataset,\n",
    "    data_preparation,\n",
    ")\n",
    "from recbole.utils import (\n",
    "    get_model,\n",
    "    init_seed,\n",
    ")\n",
    "import json\n",
    "import math\n",
    "# IMPORTANT: Specify the path to your pre-trained model file.\n",
    "# This path should be relative to the project root directory.\n",
    "MODEL_FILE = './saved/path_to_your_tmall_model.pth' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2020)\n",
    "np.random.seed(2020)\n",
    "random.seed(2020)\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.manual_seed_all(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad46d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and dataset from the checkpoint\n",
    "print(\"Loading model and data from checkpoint...\")\n",
    "checkpoint = torch.load(MODEL_FILE)\n",
    "config = checkpoint['config']\n",
    "init_seed(config['seed'], config['reproducibility']) # Ensure reproducibility\n",
    "dataset = create_dataset(config)\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "model = get_model(config[\"model\"])(config, train_data._dataset).to(config[\"device\"])\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.load_other_parameter(checkpoint.get(\"other_parameter\"))\n",
    "print(\"Model and data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d925d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_field = train_data._dataset.uid_field\n",
    "iid_field = train_data._dataset.iid_field\n",
    "user_interactions = pd.Series(train_data._dataset.inter_feat[uid_field].numpy()).value_counts()\n",
    "item_interactions = pd.Series(train_data._dataset.inter_feat[iid_field].numpy()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 10, 15, 25, 40, 60, 100, 150, np.inf]\n",
    "labels = ['0-9', '10-15', '16-25', '26-40', '41-60', '61-100', '101-150', '>150']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd537652",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_bins = [0, 5, 10, 15, 20, 30, 50, 75, 100,  200, 400, np.inf]\n",
    "item_labels = ['0-5', '6-9', '10-15', '16-20', '21-30', '31-50', '51-75', '76-100', '101-200', '201-400', '>400']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User interaction distribution statistics\n",
    "user_counts = pd.cut(user_interactions, bins=bins, labels=labels).value_counts()\n",
    "user_counts = user_counts.sort_index()  # Sort by interval order\n",
    "\n",
    "# Item interaction distribution statistics\n",
    "item_counts = pd.cut(item_interactions, bins=item_bins, labels=item_labels).value_counts()\n",
    "item_counts = item_counts.sort_index()  # Sort by interval order\n",
    "\n",
    "# Print detailed distribution data\n",
    "print(\"User interaction distribution:\")\n",
    "print(user_counts)\n",
    "print(\"\\nItem interaction distribution:\")\n",
    "print(item_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd82f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_interaction_dict={}\n",
    "for low_line in item_bins:\n",
    "    item_interaction_dict[str(low_line)+'-'] = []\n",
    "    for i_id, count in item_interactions.items():\n",
    "        if count <= low_line:\n",
    "            item_interaction_dict[str(low_line)+'-'].append(i_id)\n",
    "for high_line in item_bins:\n",
    "    item_interaction_dict[str(high_line)+'+'] = []\n",
    "    for i_id, count in item_interactions.items():\n",
    "        if count >= high_line:\n",
    "            item_interaction_dict[str(high_line)+'+'].append(i_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_repe_clustermean(low_line, high_line):\n",
    "    # Get item IDs\n",
    "    low_line_id = list(item_interaction_dict[str(low_line)+'-'])\n",
    "    high_line_id = list(item_interaction_dict[str(high_line)+'+'])\n",
    "    \n",
    "    # Early return if no data\n",
    "    if len(low_line_id) < 10 or len(high_line_id) < 10:\n",
    "        return\n",
    "    \n",
    "    # Batch process embeddings\n",
    "    with torch.no_grad():\n",
    "        low_ids = torch.tensor(low_line_id).to(config['device'])\n",
    "        high_ids = torch.tensor(high_line_id).to(config['device'])\n",
    "        \n",
    "        low_line_embeddings = model.get_item_embedding(low_ids).cpu().numpy()\n",
    "        high_line_embeddings = model.get_item_embedding(high_ids).cpu().numpy()\n",
    "\n",
    "                # Add the extra dimension in the middle\n",
    "        low_line_embeddings = np.expand_dims(low_line_embeddings, axis=1)\n",
    "        high_line_embeddings = np.expand_dims(high_line_embeddings, axis=1)\n",
    "        \n",
    "\n",
    "    print(low_line, high_line)\n",
    "    print(np.array(low_line_embeddings).shape)\n",
    "    print(np.array(high_line_embeddings).shape)\n",
    "\n",
    "    # Determine sizes\n",
    "    total_num = min(len(low_line_embeddings), len(high_line_embeddings))\n",
    "    train_num = math.ceil(int(total_num * 0.8))\n",
    "    test_num = total_num - train_num\n",
    "\n",
    "    # Shuffle once\n",
    "    np.random.shuffle(low_line_embeddings)\n",
    "    np.random.shuffle(high_line_embeddings)\n",
    "\n",
    "    # More efficient implementation with interleaving pattern\n",
    "    # Create train data with alternating pattern (low, high, low, high...)\n",
    "    train_low = low_line_embeddings[:train_num]\n",
    "    train_high = high_line_embeddings[:train_num]\n",
    "    # Interleave arrays - note the shape includes the extra dimension now\n",
    "    train_data = np.empty((2*train_num, train_low.shape[1], train_low.shape[2]), dtype=train_low.dtype)\n",
    "    train_data[0::2] = train_low  # Even indices for low popularity\n",
    "    train_data[1::2] = train_high  # Odd indices for high popularity\n",
    "    # Similar interleaved pattern for labels\n",
    "    train_labels = np.zeros(2*train_num)\n",
    "    train_labels[1::2] = 1  # Set odd indices to 1 (high popularity)\n",
    "\n",
    "    # Create test data with alternating pattern\n",
    "    test_low = low_line_embeddings[train_num:total_num]\n",
    "    test_high = high_line_embeddings[train_num:total_num]\n",
    "    # Interleave arrays - note the shape includes the extra dimension now\n",
    "    test_data = np.empty((2*test_num, test_low.shape[1], test_low.shape[2]), dtype=test_low.dtype)\n",
    "    test_data[0::2] = test_low  # Even indices for low popularity\n",
    "    test_data[1::2] = test_high  # Odd indices for high popularity\n",
    "    # Similar interleaved pattern for labels\n",
    "    test_labels = np.zeros(2*test_num)\n",
    "    test_labels[1::2] = 1  # Set odd indices to 1 (high popularity)\n",
    "\n",
    "    # Convert to torch tensors once\n",
    "    train_data = torch.tensor(train_data)\n",
    "    test_data = torch.tensor(test_data)\n",
    "    train_label = torch.tensor(train_labels)\n",
    "    test_label = torch.tensor(test_labels)\n",
    "\n",
    "\n",
    "    pca_rep_reader = PCARepReader(contrast=True)\n",
    "    rep_direction = pca_rep_reader.get_rep_directions(train_data)\n",
    "    print(rep_direction)\n",
    "    # direction_signs = pca_rep_reader.get_signs(train_data, train_label)\n",
    "    # print(direction_signs)\n",
    "    test_result = pca_rep_reader.test_accuracy(test_data)\n",
    "    results_val = {layer:{} for layer in range(0, 5)}\n",
    "    for layer in range(0, 1):\n",
    "        # Get the test results for the current layer\n",
    "        layer_results = test_result[layer]\n",
    "        \n",
    "        # Calculate number of complete pairs\n",
    "        n_pairs = test_data.shape[0] // 2\n",
    "        max_idx = 2 * n_pairs\n",
    "        \n",
    "        # First comparison: even indices with their following odd indices\n",
    "        even_indices = np.arange(0, max_idx, 2)\n",
    "        odd_indices = np.arange(1, max_idx, 2)\n",
    "        \n",
    "        # Get values at these indices\n",
    "        even_values = layer_results[even_indices]\n",
    "        odd_values = layer_results[odd_indices]\n",
    "        \n",
    "        # Second comparison: odd indices with next even indices (excluding last odd index)\n",
    "        next_indices = np.arange(2, max_idx, 2)\n",
    "        next_values = np.zeros_like(odd_values)\n",
    "        next_values[:-1] = layer_results[next_indices]  # All except last\n",
    "        \n",
    "        # First comparison: even <= odd (1 if true, 0 if false)\n",
    "        first_comparison = (even_values - odd_values <= 0).astype(int)\n",
    "        \n",
    "        # Second comparison: odd >= next (1 if true, 0 if false)\n",
    "        second_comparison = np.zeros_like(first_comparison)\n",
    "        second_comparison[:-1] = (odd_values[:-1] - next_values[:-1] >= 0).astype(int)\n",
    "        \n",
    "        # Combine results (flatten to match the original implementation)\n",
    "        result = np.zeros(2 * len(first_comparison) - 1)\n",
    "        result[0::2] = first_comparison  # Even positions for first comparison\n",
    "        result[1::2] = second_comparison[:-1]  # Odd positions for second comparison\n",
    "        \n",
    "        # Calculate final result\n",
    "        results_val[layer] = np.mean(result)\n",
    "        print(f\"{layer}: {results_val[layer]}\")\n",
    "        print(\"=====\")\n",
    "    return rep_direction, results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for low_line in item_bins:\n",
    "    for high_line in item_bins:\n",
    "        if low_line < high_line:\n",
    "            popularity_repe_clustermean(low_line, high_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Define base directory for saving results relative to the project root\n",
    "DIRECTION_BASE_DIR = \"./e_pop_saved/\" \n",
    "# Define and create a dataset-specific subdirectory\n",
    "DATASET_NAME = 'tmall' \n",
    "DIRECTION_BASE_DIR_ML = os.path.join(DIRECTION_BASE_DIR, DATASET_NAME)\n",
    "os.makedirs(DIRECTION_BASE_DIR_ML, exist_ok=True)\n",
    "os.makedirs(DIRECTION_BASE_DIR_ML, exist_ok=True)\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "LOG_FILE_PATH = os.path.join(DIRECTION_BASE_DIR_ML, f\"rep_direction_item_tmall_{TIMESTAMP}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def convert_to_serializable(value):\n",
    "    \"\"\"Recursively converts values to JSON-serializable types.\"\"\"\n",
    "    # Handle pandas Series\n",
    "    if isinstance(value, pd.Series):\n",
    "        # Convert pandas Series to dictionary\n",
    "        try:\n",
    "            # First approach: iterate through items\n",
    "            return {str(k): convert_to_serializable(v) for k, v in value.items()}\n",
    "        except Exception:\n",
    "            # Fallback: use to_dict() method which is more reliable\n",
    "            return {str(k): convert_to_serializable(v) for k, v in value.to_dict().items()}\n",
    "    # Handle scalar types\n",
    "    elif isinstance(value, torch.Tensor):\n",
    "        return value.cpu().tolist()\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        return value.tolist()\n",
    "    elif isinstance(value, (np.int_, np.intc, np.intp, np.int8,\n",
    "                          np.int16, np.int32, np.int64, np.uint8,\n",
    "                          np.uint16, np.uint32, np.uint64)):\n",
    "        return int(value)\n",
    "    elif isinstance(value, (np.float_, np.float16, np.float32,\n",
    "                          np.float64)):\n",
    "        return float(value)\n",
    "    elif isinstance(value, OrderedDict):\n",
    "        # For OrderedDict, maintain order within the dict representation\n",
    "        return {k: convert_to_serializable(v) for k, v in value.items()}\n",
    "    # Handle collections recursively\n",
    "    elif isinstance(value, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in value.items()}\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        return [convert_to_serializable(item) for item in value]\n",
    "    # Return everything else as is\n",
    "    return value\n",
    "\n",
    "def log_message(key, value):\n",
    "    \"\"\"Stores a log entry (key-value pair) with timestamp for later saving.\"\"\"\n",
    "    # Convert value to JSON-serializable format\n",
    "    serializable_value = convert_to_serializable(value)\n",
    "\n",
    "    log_entry = {\n",
    "        # \"timestamp\": datetime.now().isoformat()\n",
    "        \"key\": key,\n",
    "        \"value\": serializable_value\n",
    "    }\n",
    "    log_data.append(log_entry)\n",
    "\n",
    "    print(f\"LOG [{key}]: {serializable_value}\")\n",
    "\n",
    "def save_log_to_json():\n",
    "    \"\"\"Saves all collected log messages sequentially to a JSON file.\"\"\"\n",
    "    try:\n",
    "        # Extra safety: convert everything one more time to ensure it's serializable\n",
    "        serializable_data = convert_to_serializable(log_data)\n",
    "        with open(LOG_FILE_PATH, 'w') as f:\n",
    "            json.dump(serializable_data, f, indent=4)\n",
    "        print(f\"Log data successfully saved sequentially to {LOG_FILE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving log data to JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e01d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_direction, results_val = popularity_repe_clustermean(5, 100) # you can change the parameters here\n",
    "log_message(\"rep_direction_clustermean_low_high_line\", [5, 100])\n",
    "log_message(\"rep_direction_clustermean\", rep_direction)\n",
    "log_message(\"results_val_clustermean\", results_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_log_to_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole1.2.1",
   "language": "python",
   "name": "recbole"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
